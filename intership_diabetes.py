# -*- coding: utf-8 -*-
"""Intership_diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14QDc73fbzSDaCjukzaXoU5jj3ChEq8gm
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("uciml/pima-indians-diabetes-database")

print("Path to dataset files:", path)

import os

os.listdir(path)

import pandas as pd

file_path = os.path.join(path, "diabetes.csv")
df = pd.read_csv(file_path)

df.head()

df.shape          # rows, columns

df.info()

df.describe()

df.isnull().sum()

# identify zero null
(df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] == 0).sum()

# change zero to null
import numpy as np
cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']
df[cols] = df[cols].replace(0, np.nan)
df

df.isnull().sum()

df['Glucose'].fillna(df['Glucose'].mean(), inplace=True)
df['BloodPressure'].fillna(df['BloodPressure'].mean(), inplace=True)
df['SkinThickness'].fillna(df['SkinThickness'].mean(), inplace=True)
df['Insulin'].fillna(df['Insulin'].mean(), inplace=True)
df['BMI'].fillna(df['BMI'].mean(), inplace=True)

df

df.isnull().sum()

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='Outcome', data=df)
plt.show()

# no of pregnencies and skin thickness in not required
df = df.drop(columns=["Pregnancies", "SkinThickness"])

X = df.drop('Outcome', axis=1)
y = df['Outcome']

df

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
X_scaled.head()

X_scaled.isnull().sum()

# split to train and test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y   #for imbalanced data
)

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train, y_train)

from sklearn.metrics import classification_report, roc_auc_score

y_pred_lr = lr_model.predict(X_test)
y_prob = lr_model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred_lr))
print("AUC:", roc_auc_score(y_test, y_prob))





lr_model_bal = LogisticRegression(
    class_weight='balanced',
    random_state=42,
    max_iter=1000
)

lr_model_bal.fit(X_train, y_train)

from sklearn.metrics import classification_report, roc_auc_score

y_pred_bal = lr_model_bal.predict(X_test)
y_prob = lr_model_bal.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred_bal))
print("AUC:", roc_auc_score(y_test, y_prob))



from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

from sklearn.linear_model import LogisticRegression

lr_model_sm = LogisticRegression(max_iter=1000, random_state=42)
lr_model_sm.fit(X_train_sm, y_train_sm)

y_pred_smote = lr_model_sm.predict(X_test)

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred_smote)
print("Accuracy (SMOTE + LR):", acc)

from sklearn.metrics import classification_report

print("LR")
print(classification_report(y_test, y_pred_lr))

print("Balanced LR")
print(classification_report(y_test, y_pred_bal))

print("SMOTE + LR")
print(classification_report(y_test, y_pred_smote))



from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    random_state=42
)

lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

acc_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Accuracy:", acc_lr)

from sklearn.svm import SVC

svm = SVC(
    kernel='rbf',
    class_weight='balanced',
    probability=True,
    random_state=42
)

svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

acc_svm = accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy:", acc_svm)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=300,
    class_weight='balanced',
    random_state=42
)

rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

acc_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Accuracy:", acc_rf)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.utils.class_weight import compute_sample_weight

sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=y_train
)

gb = GradientBoostingClassifier(random_state=42)

gb.fit(X_train, y_train, sample_weight=sample_weights)
y_pred_gb = gb.predict(X_test)

acc_gb = accuracy_score(y_test, y_pred_gb)
print("Gradient Boosting Accuracy:", acc_gb)

from xgboost import XGBClassifier
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.unique(y_train)
weights = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=y_train
)

class_weight_dict = dict(zip(classes, weights))

xgb = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=4,
    scale_pos_weight=class_weight_dict[1] / class_weight_dict[0],
    eval_metric='logloss',
    random_state=42
)

xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

acc_xgb = accuracy_score(y_test, y_pred_xgb)
print("XGBoost Accuracy:", acc_xgb)

results = pd.DataFrame({
    "Model": [
        "Logistic Regression",
        "SVM",
        "Random Forest",
        "Gradient Boosting",
        "XGBoost"
    ],
    "Accuracy": [
        acc_lr,
        acc_svm,
        acc_rf,
        acc_gb,
        acc_xgb
    ]
})

results



#Deep learning models
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

class_weight_dict = dict(zip(np.unique(y_train), class_weights))
class_weight_dict

#MLP
model1 = Sequential([
    Dense(24, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(12, activation='relu'),
    Dense(1, activation='sigmoid')
])

model1.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history1 = model1.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    class_weight=class_weight_dict,
    verbose=0
)

y_pred_dl1 = (model1.predict(X_test) > 0.5).astype(int)
acc_dl1 = accuracy_score(y_test, y_pred_dl1)
print("DL Model 1 Accuracy:", acc_dl1)

#Regularized Deep Neural Network
model2 = Sequential([
    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(16, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model2.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history2 = model2.fit(
    X_train, y_train,
    epochs=70,
    batch_size=32,
    validation_split=0.2,
    class_weight=class_weight_dict,
    verbose=0
)

y_pred_dl2 = (model2.predict(X_test) > 0.5).astype(int)
acc_dl2 = accuracy_score(y_test, y_pred_dl2)
print("DL Model 2 Accuracy:", acc_dl2)

#Cost-Sensitive Neural Network
model3 = Sequential([
    Dense(24, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(12, activation='relu'),
    Dense(1, activation='sigmoid')
])

model3.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history3 = model3.fit(
    X_train, y_train,
    epochs=60,
    batch_size=32,
    validation_split=0.2,
    class_weight=class_weight_dict,
    verbose=0
)

y_pred_dl3 = (model3.predict(X_test) > 0.5).astype(int)
acc_dl3 = accuracy_score(y_test, y_pred_dl3)
print("DL Model 3 Accuracy:", acc_dl3)

import pandas as pd

dl_results = pd.DataFrame({
    "Deep Learning Model": [
        "Basic MLP",
        "Regularized MLP",
        "Cost-Sensitive MLP"
    ],
    "Accuracy": [
        acc_dl1,
        acc_dl2,
        acc_dl3
    ]
})

dl_results



# Check for overfitting
from sklearn.metrics import accuracy_score

# Training accuracy
y_train_pred = rf.predict(X_train)
train_acc = accuracy_score(y_train, y_train_pred)

# Test accuracy
y_test_pred = rf.predict(X_test)
test_acc = accuracy_score(y_test, y_test_pred)

print("Train Accuracy:", train_acc)
print("Test Accuracy :", test_acc)

from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_scores = cross_val_score(
    rf, X, y,
    cv=cv,
    scoring='accuracy'
)

print("CV Accuracy scores:", cv_scores)
print("Mean CV Accuracy :", cv_scores.mean())
print("Std CV Accuracy  :", cv_scores.std())



rf_tuned = RandomForestClassifier(
    n_estimators=300,
    max_depth=5,
    min_samples_leaf=10,
    max_features='sqrt',
    class_weight='balanced',
    random_state=42
)

rf_tuned.fit(X_train, y_train)

from sklearn.metrics import classification_report, roc_auc_score

y_test_pred = rf_tuned.predict(X_test)
y_prob = rf_tuned.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_test_pred))
print("AUC:", roc_auc_score(y_test, y_prob))

# Confirm
y_test_pred = rf_tuned.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_test_pred)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_test_pred))

import joblib

joblib.dump(rf_tuned, "diabetes_model.pkl")
joblib.dump(scaler, "scaler.pkl")



